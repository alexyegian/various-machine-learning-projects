{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9hKYBUd11Kz"
      },
      "source": [
        "HW1: (Total 60 points)\n",
        "Coding 50 points\n",
        "report 10 points\n",
        "\n",
        "Submission:\n",
        "Submit your code as ipynb and report as pdf file through canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EQJvlQbc2Zo"
      },
      "source": [
        "Loading data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dbCQODFc4MG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093dc105-5db2-4ede-fe0f-5928e5eb95d6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Imported the modules.\")\n",
        "\n",
        "test_path = \"california_housing_test.csv\"\n",
        "train_path = \"california_housing_train.csv\"\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "#print(train_df)\n",
        "# Shuffle the examples\n",
        "np.random.seed(0) # fix the random seed\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index))\n",
        "# Processing training set\n",
        "X = train_df[['longitude', 'latitude','street_name']]\n",
        "Y = train_df['median_house_value']\n",
        "# Processing testing set\n",
        "X_test = test_df[['longitude', 'latitude','street_name']]\n",
        "Y_test = test_df['median_house_value']\n",
        "np_x = X.to_numpy()\n",
        "np_y = Y.to_numpy()\n",
        "np_test_x = X_test.to_numpy()\n",
        "np_test_y = Y_test.to_numpy()\n",
        "#print(\"X\")\n",
        "#print(np_x.shape)\n",
        "#print(\"Y\")\n",
        "#print(np_y.shape)\n",
        "#print(\"X TEST\")\n",
        "#print(np_test_x.shape)\n",
        "#print(\"Y TEST\")\n",
        "#print(np_test_y.shape)\n",
        "#print(np_test_x)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Imported the modules.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8FrQ-MgczZW"
      },
      "source": [
        "**Question 1** [10-points]: Do k fold split to your data. Split the data into 10 parts\n",
        "\n",
        "Note that Don't use any sort of library like Sk-learn\n",
        "\n",
        "Evaluation metric: After the split you suppose to reunion them and get back the same original set. If not, there is a bug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWy7pZXjdKvm"
      },
      "source": [
        "def eval_kfold (train_index, val_index):\n",
        "    #print(\"EVAL K FOLD\")\n",
        "    all_indx = np.concatenate((train_index, val_index))\n",
        "    all_indx.sort()\n",
        "    original = np.arange(all_indx.shape[0])\n",
        "    #print(\"ORIGINAL: \",original)\n",
        "    #print(\"ALL INDX: \", all_indx)\n",
        "    for i in range(original.shape[0]):\n",
        "        if all_indx[i] != original[i]: print(\"ERROR: Your K-fold split has a bug\")\n",
        "    #print(\"ended\")\n",
        "    return all_indx"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka5cl8JodOlB"
      },
      "source": [
        "This is how it suppose to be like using sk-learn \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7KgW8XL1MB1"
      },
      "source": [
        "#print(\"X: \", X)\n",
        "#print(\"NP X: \", np_x.shape)\n",
        "kf = KFold(n_splits=10,shuffle=True)\n",
        "kf.get_n_splits(np_x)\n",
        "#Takes 1/10 of the thing, and assigns those indeces to val indeces, and the other 9/10 are train indeces\n",
        "for train_index, val_index in kf.split(np_x):\n",
        "    #print(\"TRAIN INDEX: \", train_index),\n",
        "    #print(\"VAL INDEX: \", val_index)\n",
        "    eval_kfold (train_index, val_index)\n",
        "    X_train, X_val = np_x[train_index], np_x[val_index]\n",
        "    y_train, y_val = np_y[train_index], np_y[val_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s04CrVE8dTSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2024322e-bbe6-4983-a70e-08b5eaf48544"
      },
      "source": [
        "#### Your code starts here###\n",
        "\n",
        "#BASIC VERSION, THIS ONE DOESN'T RANDOMIZE THE INDECES\n",
        "np_x = X.to_numpy()\n",
        "np_y = Y.to_numpy()\n",
        "change_amount = np_x.shape[0]/10\n",
        "index_list = np.arange(np_x.shape[0])\n",
        "first_10 = np_x[:10]\n",
        "for x in range(10):\n",
        "    test_list = index_list[int(x*change_amount): int((x+1)*change_amount)]\n",
        "    train_list_ind = np.append(index_list[0:(int(x*change_amount))], index_list[int((x+1)*change_amount):])\n",
        "    eval_kfold(train_list_ind, test_list)\n",
        "    test_x = np_x[int(x*change_amount): int((x+1)*change_amount)]\n",
        "    test_y = np_y[int(x*change_amount): int((x+1)*change_amount)]\n",
        "    train_x = np.append(np_x[0:(int(x*change_amount))], np_x[int((x+1)*change_amount):])\n",
        "    train_x.shape = (180, 3)\n",
        "    train_y = np.append(np_y[0:(int(x*change_amount))], np_y[int((x+1)*change_amount):])\n",
        "    train_y.shape = (180, 1)\n",
        "    print(\"TEST LIST\")\n",
        "    print(test_list)\n",
        "    print(\"TRAIN LIST\")\n",
        "    print(train_list_ind)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST LIST\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "TRAIN LIST\n",
            "[ 20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
            "  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
            "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
            "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
            " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
            " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
            "  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
            "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
            "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
            " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
            " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
            "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
            "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
            " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
            " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  80  81  82  83  84  85  86  87  88  89  90  91\n",
            "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
            " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
            " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79 100 101 102 103 104 105 106 107 108 109\n",
            " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
            " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
            " 118 119]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 120 121 122 123 124 125 126 127\n",
            " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
            " 138 139]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            " 158 159]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n",
            " 178 179]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
            "TEST LIST\n",
            "[180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199]\n",
            "TRAIN LIST\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXCmlRindadK"
      },
      "source": [
        "**Question 2** [10-points]: Encode street_name feature into one hot incoding and print out the result.\n",
        "\n",
        "Note that Don't use any sort of library like Sk-learn\n",
        "\n",
        "Evluation metric: print out your encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzb0Sz88dd-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c283947e-7980-4254-98e9-ae4a79dfe6b0"
      },
      "source": [
        "#### Your code starts here###\n",
        "\n",
        "new_col_2 = np.zeros((np_x.shape[0], 9))\n",
        "total = np.hstack((np_x, new_col_2))\n",
        "for i in range(total.shape[0]):\n",
        "    name = total[i][2]\n",
        "    if(name == \"oakes\"):\n",
        "        total[i][3+0] = 1\n",
        "    elif(name == \"stevenson\"):\n",
        "        total[i][3+1] = 1\n",
        "    elif(name == \"crown\"):\n",
        "        total[i][3+2] = 1\n",
        "    elif(name == \"merrill\"):\n",
        "        total[i][3+3] = 1\n",
        "    elif(name == \"nine\"):\n",
        "        total[i][3+4] = 1\n",
        "    elif(name == \"ten\"):\n",
        "        total[i][3+5] = 1\n",
        "    elif(name == \"rc\"):\n",
        "        total[i][3+6] = 1\n",
        "    elif(name == \"kresge\"):\n",
        "        total[i][3+7] = 1\n",
        "    elif(name == \"porter\"):\n",
        "        total[i][3+8] = 1\n",
        "print(\"TOTAL 10 ELEMENTS: \")\n",
        "print(total[0:10])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOTAL 10 ELEMENTS: \n",
            "[[-114.66 32.74 'oakes' 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
            " [-116.25 33.75 'kresge' 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0.0]\n",
            " [-115.69 32.79 'nine' 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0]\n",
            " [-115.58 32.78 'merrill' 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 0.0]\n",
            " [-116.3 33.68 'kresge' 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0.0]\n",
            " [-116.32 34.14 'porter' 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1]\n",
            " [-114.58 33.63 'oakes' 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
            " [-116.18 33.69 'rc' 0.0 0.0 0.0 0.0 0.0 0.0 1 0.0 0.0]\n",
            " [-114.61 34.84 'oakes' 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
            " [-116.21 33.75 'rc' 0.0 0.0 0.0 0.0 0.0 0.0 1 0.0 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcKGaobsdiFL"
      },
      "source": [
        "**Question 3** [10-points]: Feture crossing\n",
        "\n",
        "Given the bin version of the feature longitude and latitude. Please build the feature cross version of these two feature and print it out\n",
        "\n",
        "Note that Don't use any sort of library like Sk-learn\n",
        "\n",
        "Evluation metric: print out your encoding. Explain what you did and what does it means\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTv2q_ladfkH"
      },
      "source": [
        "def create_one_hot_features(df, feature_name, bin_number):\n",
        " # print(bin_number)\n",
        "  df[feature_name+'_bin'] = pd.cut(df[feature_name], bins=bin_number)\n",
        "  df_encoded = pd.get_dummies(df, columns=[feature_name+'_bin'])\n",
        "  return df_encoded\n",
        "\n",
        "# Here is the given bins\n",
        "bins_longitude = pd.IntervalIndex.from_tuples([(-125, -121), (-121, -117), (-117, -113)])\n",
        "bins_latitude = pd.IntervalIndex.from_tuples([(32,33),(33,34), (34,35),(35,36)])\n",
        "#-114.66     32.74\n",
        "\n",
        "# One hot encoding using sk-learn\n",
        "X_longitude_encoded = create_one_hot_features(X, 'longitude', bins_longitude)\n",
        "X_test_longitude_encoded = create_one_hot_features(X_test, 'longitude', bins_longitude)\n",
        "#print(X_longitude_encoded)\n",
        "np_lat_x = X_longitude_encoded.to_numpy()\n",
        "#print(np_lat_x[0][3])\n",
        "X_latitude_encoded = create_one_hot_features(X, 'latitude', bins_latitude)\n",
        "X_test_latitude_encoded = create_one_hot_features(X_test, 'latitude', bins_latitude)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9rsxiDweGuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab66bbc8-884a-49df-9759-c663d5e764bc"
      },
      "source": [
        "# Your code starts here\n",
        "#Make 12 new rows\n",
        "#long x, lat y, index of new column is x*(num of longs(3))+y\n",
        "def create_crossed_feature(df, df_start_ind, long, num_long, lat, num_lat, start_ind):\n",
        "  num_of_each = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  for x in range(df.shape[0]):\n",
        "    cur_pos = df_start_ind\n",
        "    for y in range(num_long):\n",
        "      if long[x][y+start_ind] == 1:\n",
        "          cur_pos += y*num_lat\n",
        "    for y in range(num_lat):\n",
        "      if lat[x][y+start_ind] == 1:\n",
        "          cur_pos += y\n",
        "    num_of_each[cur_pos] += 1\n",
        "    df[x][cur_pos] = 1\n",
        "\n",
        "new_np_x = np.copy(np_x)\n",
        "crossed_features = np.zeros((new_np_x.shape[0], 13))\n",
        "np_lat_x = X_latitude_encoded.to_numpy()\n",
        "np_long_x = X_longitude_encoded.to_numpy()\n",
        "for x in range(crossed_features.shape[0]):\n",
        "    crossed_features[x][0] = 1 \n",
        "create_crossed_feature(crossed_features, 1, np_long_x, 3, np_lat_x, 4, 4)\n",
        "print(np_x[0])\n",
        "print(\"CROSSED FEATURES FIRST 10\")\n",
        "print(crossed_features[0:10])\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-114.66 32.74 'oakes']\n",
            "CROSSED FEATURES FIRST 10\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKIoIlWDeeIl"
      },
      "source": [
        "**Question 4** [10-points]: Linear Regression\n",
        "\n",
        "From question 3 we now have 13 parameters 3*4+1. 3 from longitude, 4 from latitude and 1 bias. \n",
        "\n",
        "Do a linear fit to predict the house price by using these 13 parameters \n",
        "\n",
        "Evluation metric: print out the accuracy when you test the linear model on test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwS5GD3Bed35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b5a5af-341e-47f8-dbcd-3277984bea7b"
      },
      "source": [
        "# Refer to in class exercise as your starter code\n",
        "# The in class we only deal with low dimention input case. Now try to make it \n",
        "# works in this case.\n",
        "# Hint: Take a look at these two functions. Feel free to reuse some of the \n",
        "# exercise code we provided. \n",
        "\"\"\"\n",
        "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
        "    gradient steps with learning rate `alpha`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataset of shape (m x n+1).\n",
        "    \n",
        "    y : array_like\n",
        "        Value at given features. A vector of shape (m, ).\n",
        "    \n",
        "    theta : array_like\n",
        "        Initial values for the linear regression parameters. \n",
        "        A vector of shape (n+1, ).\n",
        "    \n",
        "    alpha : float\n",
        "        The learning rate.\n",
        "    \n",
        "    num_iters : int\n",
        "        The number of iterations for gradient descent.\n",
        "\"\"\"\n",
        "def find_actual_vals(y_data, crossed_data):\n",
        "    num_of_each = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    med_price = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    for x in range(y_data.shape[0]):\n",
        "            if math.isnan(y_data[x]):\n",
        "               continue\n",
        "            value_vec = y_data[x]*crossed_data[x]\n",
        "            for y in range(value_vec.shape[0]):\n",
        "                if y != 0:  \n",
        "                    num_of_each[y-1] += crossed_data[x][y]\n",
        "                    med_price[y-1] += value_vec[y]\n",
        "    for x in range(12):\n",
        "        med_price[x] = med_price[x] / num_of_each[x]\n",
        "    print(\"MED PRICE\")\n",
        "    print(med_price)\n",
        "    print(\"NUM EACH\")\n",
        "    print(num_of_each)\n",
        "iterations = 1500\n",
        "alpha = 0.01\n",
        "theta = np.zeros(crossed_features.shape[1])\n",
        "med_vals = np.zeros(crossed_features.shape[0])\n",
        "find_actual_vals(np_y, crossed_features)\n",
        "def computeCost(X, y, theta, j):\n",
        "    J = np.sum(((X @ theta) - y) * X[:,j])\n",
        "    return J\n",
        "\n",
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    m = y.shape[0]\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        theta_tmp = []    \n",
        "        for j in range(len(theta)): # partial derivative \n",
        "            gradient = (alpha/m) * computeCost(X, y, theta, j)\n",
        "            new_theta = theta[j] - gradient\n",
        "            theta_tmp.append(new_theta)        \n",
        "        theta = theta_tmp\n",
        "    print(\"THETA FINAL:\")\n",
        "    print(theta)\n",
        "    return theta\n",
        "\n",
        "end_theta = gradientDescent(crossed_features, np_y, theta, alpha, iterations)\n",
        "\n",
        "crossed_features_test = np.zeros((np_test_x.shape[0], 13))\n",
        "np_lat_x = X_latitude_encoded.to_numpy()\n",
        "np_long_x = X_longitude_encoded.to_numpy()\n",
        "for x in range(crossed_features_test.shape[0]):\n",
        "    crossed_features_test[x][0] = 1 \n",
        "np_lat_test_x = X_test_latitude_encoded.to_numpy()\n",
        "np_long_test_x = X_test_longitude_encoded.to_numpy()\n",
        "create_crossed_feature(crossed_features_test, 1, np_long_test_x, 3, np_lat_test_x, 4, 4)\n",
        "find_actual_vals(np_test_y, crossed_features_test)\n",
        "#*(275000.0-(20910+58181))+ (194400-58181)*14+(125016-58181)*6+(201864-58181)*14+(275833-58181)*12+(112600-58181)*2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MED PRICE\n",
            "[nan, nan, nan, nan, nan, nan, nan, nan, 79155.55555555556, 91647.77777777778, 62774.07407407407, 50200.0, 0]\n",
            "NUM EACH\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 81.0, 90.0, 27.0, 2.0, 0]\n",
            "THETA FINAL:\n",
            "[58181.376745520436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 20910.18572742563, 33397.12240677291, 4660.114788089822, -786.0461767679182]\n",
            "MED PRICE\n",
            "[194400.0, nan, nan, nan, 125016.66666666667, 201864.2857142857, 275833.4166666667, 112600.0, 275000.0, nan, nan, nan, 0]\n",
            "NUM EACH\n",
            "[14.0, 0.0, 0.0, 0.0, 6.0, 14.0, 12.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}